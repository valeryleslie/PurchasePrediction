{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n",
    "import re\n",
    "import keras\n",
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "seed = 50\n",
    "\n",
    "from numpy import mean\n",
    "from sklearn import decomposition\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_classif as MIC\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import xgboost as xgb\n",
    "from dython.nominal import associations\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, roc_auc_score, roc_curve, f1_score\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "from plotnine import *\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from sklearn.utils import class_weight\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = associations(X_mi_mob4, ax = ax, cmap = \"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 15))\n",
    "# define the mask to set the values in the upper triangle to True\n",
    "mask = np.triu(np.ones_like(r[\"corr\"].round(2), dtype=bool))\n",
    "heatmap = sns.heatmap(r[\"corr\"].round(2), mask=mask, vmin=-1, vmax=1, annot=False, cmap='BrBG')\n",
    "heatmap.set_title('Triangle Correlation Heatmap', fontdict={'fontsize':18}, pad=16);\n",
    "fig = heatmap.get_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_columns:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "X = df.drop('is_purchase', axis=1)  # drop the target variable from the features\n",
    "y = df['is_purchase']  # select the target variable as the labels\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)\n",
    "\n",
    "# convert the data into a DMatrix format\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test, enable_categorical=True)\n",
    "\n",
    "# define the XGBoost parameters\n",
    "params = {\n",
    "    'max_depth': 3,\n",
    "    'eta': 0.1,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'error'\n",
    "}\n",
    "\n",
    "# Train the model with early stopping\n",
    "model = xgb.train(params, dtrain, num_boost_round=1000, evals=[(dtest, 'validation')], early_stopping_rounds=50)\n",
    "\n",
    "# Use the number of rounds at which early stopping occurred as the optimal number of rounds\n",
    "optimal_num_rounds = model.best_iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "num = [1,2,3,4,5,6,7,8,9,10]\n",
    "for i in num:\n",
    "  # train the XGBoost model\n",
    "  xgb_model = xgb.train(params, dtrain, num_boost_round=i)\n",
    "\n",
    "  # make predictions on the test set\n",
    "  y_pred = xgb_model.predict(dtest)\n",
    "  y_pred = [1 if p > 0.5 else 0 for p in y_pred]\n",
    "\n",
    "  # evaluate the model accuracy\n",
    "  temp = accuracy_score(y_test,y_pred)\n",
    "  acc.append(temp)\n",
    "  #print(\"XGBoost accuracy:\", temp)\n",
    "\n",
    "plt.plot(num,acc,'bx-')\n",
    "plt.xlabel('Values of num_boost_round') \n",
    "plt.ylabel('Model Accuracy') \n",
    "plt.title('Model Accuracy For Optimal num_boost_round')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model accuracy\n",
    "print(\"Optimal num rounds:\", optimal_num_rounds)\n",
    "print(\"XGBoost accuracy:\", max(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.train(params, dtrain, num_boost_round=optimal_num_rounds)\n",
    "\n",
    "# plot the feature importance\n",
    "xgb.plot_importance(xgb_model, max_num_features=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Matplotlib defaults\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=14,\n",
    "    titlepad=10,\n",
    ")\n",
    "# Load data\n",
    "\n",
    "# Utility functions\n",
    "def make_mi_scores(X, y):\n",
    "    X = X.copy()\n",
    "    for colname in X.select_dtypes([\"object\", \"category\"]):\n",
    "        X[colname], _ = X[colname].factorize()\n",
    "    # All discrete features should now have integer dtypes\n",
    "    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "def plot_mi_scores(scores):\n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width, scores)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mutual Information \n",
    "\n",
    "mi_scores = make_mi_scores(data[columns], data[is_purchase])\n",
    "mi_scores.head()\n",
    "\n",
    "plt.figure(dpi=100, figsize=(8, 5))\n",
    "plot_mi_scores(mi_scores.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA()\n",
    "pca.n_components = 4\n",
    "pca_data = pca.fit_transform(data)\n",
    "\n",
    "pca_data = np.vstack((pca_data.T)).T\n",
    "pca_df = pd.DataFrame(data=pca_data, columns=(\"1st_principal\", \"2nd_principal\", \"3rd_principal\", \"label\"))\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "explained_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,10)).gca(projection='3d')\n",
    "s = fig.scatter(\n",
    "    xs=pca_df[\"1st_principal\"], \n",
    "    ys=pca_df[\"2nd_principal\"], \n",
    "    zs=pca_df[\"3rd_principal\"], \n",
    "    c=pca_df[\"label\"], \n",
    "    cmap='tab10'\n",
    ")\n",
    "plt.legend(handles=s.legend_elements()[0], labels=[0,1,2,3,4,5,6,7,8,9], loc='best')\n",
    "fig.set_xlabel('1st_principal')\n",
    "fig.set_ylabel('2nd_principal')\n",
    "fig.set_zlabel('3rd_principal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(data)\n",
    "plt.plot(pca.explained_variance_ratio_.cumsum(), lw=3, color='#087E8B')\n",
    "plt.title('Cumulative explained variance by number of principal components', size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = numeric\n",
    "loadings = pd.DataFrame(\n",
    "    data=pca.components_.T.dot(np.diag(np.sqrt(pca.explained_variance_))), \n",
    "    columns=[f'PC{i}' for i in range(1, pca.n_components_ + 1)],\n",
    "    index=columns\n",
    ")\n",
    "loadings.head(33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc1_loadings = loadings.sort_values(by='PC1', ascending=False)[['PC1']]\n",
    "pc1_loadings = pc1_loadings.reset_index()\n",
    "pc1_loadings.columns = ['Attribute', 'CorrelationWithPC1']\n",
    "\n",
    "plt.bar(x=pc1_loadings['Attribute'], height=pc1_loadings['CorrelationWithPC1'], color='#087E8B')\n",
    "plt.title('PCA loading scores (first principal component) - 4 clicks', size=20)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.drop(\"is_purchase\", axis = 1)\n",
    "X = df.loc[:, ~df.columns.isin(categorical_columns)]\n",
    "y = clicks4.is_purchasor\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)\n",
    "\n",
    "# model\n",
    "model = LogisticRegression(class_weight = 'balanced')\n",
    "# model.compile(optimizer=optimizer,\n",
    "#               loss='sparse_categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# history = model.fit(X_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
    "# plot_history(history)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy score:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Balanced accuracy score:\", metrics.balanced_accuracy_score(y_test, y_pred))\n",
    "print(\"ROC AUC score:\", metrics.roc_auc_score(y_test, y_pred))\n",
    "print(\"Precision score:\", metrics.precision_score(y_test, y_pred))\n",
    "print(\"Sensitivity score:\", metrics.recall_score(y_test, y_pred))\n",
    "print(\"Specificity score:\", metrics.recall_score(y_test, y_pred, pos_label = 0))\n",
    "print(\"F-score:\", metrics.f1_score(y_test, y_pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_pred, probs,train_predictions, train_probs):\n",
    "    baseline = {}\n",
    "    baseline['recall']=recall_score(y_test,\n",
    "                    [1 for _ in range(len(y_test))])\n",
    "    baseline['precision'] = precision_score(y_test,\n",
    "                    [1 for _ in range(len(y_test))])\n",
    "    baseline['roc'] = 0.5\n",
    "    results = {}\n",
    "    results['recall'] = recall_score(y_test, y_pred)\n",
    "    results['precision'] = precision_score(y_test, y_pred)\n",
    "    results['roc'] = roc_auc_score(y_test, probs)\n",
    "    train_results = {}\n",
    "    train_results['recall'] = recall_score(y_train,       train_predictions)\n",
    "    train_results['precision'] = precision_score(y_train, train_predictions)\n",
    "    train_results['roc'] = roc_auc_score(y_train, train_probs)\n",
    "    for metric in ['recall', 'precision', 'roc']:  \n",
    "          print(f'{metric.capitalize()} Baseline: {round(baseline[metric], 2)} Test: {round(results[metric], 2)} Train: {round(train_results[metric], 2)}')\n",
    "     # Calculate false positive rates and true positive rates\n",
    "    base_fpr, base_tpr, _ = roc_curve(y_test, [1 for _ in range(len(y_test))])\n",
    "    model_fpr, model_tpr, _ = roc_curve(y_test, probs)\n",
    "    plt.figure(figsize = (8, 6))\n",
    "    plt.rcParams['font.size'] = 16\n",
    "    # Plot both curves\n",
    "    plt.plot(base_fpr, base_tpr, 'b', label = 'baseline')\n",
    "    plt.plot(model_fpr, model_tpr, 'r', label = 'model')\n",
    "    plt.legend();\n",
    "    plt.xlabel('False Positive Rate');\n",
    "    plt.ylabel('True Positive Rate'); plt.title('ROC Curves');\n",
    "    plt.show();\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize = False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Greens): # can change color \n",
    "    plt.figure(figsize = (10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, size = 24)\n",
    "    plt.colorbar(aspect=4)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, size = 14)\n",
    "    plt.yticks(tick_marks, classes, size = 14)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    # Label the plot\n",
    "    for i, j in itertools.product(range(cm.shape[0]),   range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), \n",
    "                 fontsize = 20,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.grid(None)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', size = 18)\n",
    "    plt.xlabel('Predicted label', size = 18)\n",
    "def encode_and_bind(original_dataframe, features_to_encode):\n",
    "    dummies = pd.get_dummies(original_dataframe[features_to_encode])\n",
    "    res = pd.concat([dummies, original_dataframe], axis=1)\n",
    "    res = res.drop(features_to_encode, axis=1)\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest With Class Weighting\n",
    "## Creation of dataset\n",
    "features_to_encode = list(X_train.select_dtypes(include = ['object']).columns) \n",
    "\n",
    "col_trans = make_column_transformer(\n",
    "                        (OneHotEncoder(handle_unknown = 'ignore'),features_to_encode),\n",
    "                        remainder = \"passthrough\"\n",
    "                        )\n",
    "# Model definition\n",
    "rf_classifier = RandomForestClassifier(\n",
    "                      min_samples_leaf=50,\n",
    "                      n_estimators=150,\n",
    "                      bootstrap=True,\n",
    "                      oob_score=True,\n",
    "                      n_jobs=-1,\n",
    "                      random_state=seed,\n",
    "                      max_features='auto', class_weight = \"balanced\")\n",
    "\n",
    "# class_weight = balanced use inverse weighting from training dataset\n",
    "# pipe = make_pipeline(col_trans, rf_classifier)\n",
    "pipe = Pipeline(steps=[('preprocessor', col_trans),\n",
    "                       ('selector', SelectFromModel(rf_classifier)),\n",
    "                       ('classifier', rf_classifier)])\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluating classfier - ACCURACY\n",
    "y_pred = pipe.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)\n",
    "print(f\"The accuracy of the model is {round(accuracy_score(y_test,y_pred),3)*100} %\")\n",
    "train_probs = pipe.predict_proba(X_train)[:,1] \n",
    "probs = pipe.predict_proba(X_test)[:, 1]\n",
    "train_predictions = pipe.predict(X_train)\n",
    "print(f'Train ROC AUC Score: {roc_auc_score(y_train, train_probs)}')\n",
    "print(f'Test ROC AUC  Score: {roc_auc_score(y_test, probs)}')\n",
    "## ROC CURVE\n",
    "evaluate_model(y_pred,probs,train_predictions,train_probs)\n",
    "\n",
    "##CONFUSION MATRIX\n",
    "# Let's plot it out\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plot_confusion_matrix(cm, classes = ['0 - Non-Purchaser', '1 - Purchaser'],\n",
    "                      title = 'Purchaser Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_Y = y_test\n",
    "test_predictions = y_pred\n",
    "print(\"Accuracy Score:\", metrics.accuracy_score(val_Y, test_predictions))\n",
    "print(\"Balanced Accuracy Score:\", metrics.balanced_accuracy_score(val_Y, test_predictions)) # Accuracy for imbalanced data\n",
    "print(\"ROC Score:\", metrics.roc_auc_score(val_Y, test_predictions))\n",
    "print(\"Precision\", metrics.precision_score(val_Y, test_predictions)) # Of the positives predicted, what percentage is truly positive\n",
    "print(\"Sensitivity\", metrics.recall_score(val_Y, test_predictions)) # Of all the positive cases, what percentage are predicted positive\n",
    "print(\"Specificity\", metrics.recall_score(val_Y, test_predictions, pos_label = 0)) # How well the model is at prediciting negative results\n",
    "print(\"F-Score\", metrics.f1_score(val_Y, test_predictions)) # The \"harmonic mean\" of precision and sensitivity. Good for imbalanced datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize = False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Greens): # can change color \n",
    "    plt.figure(figsize = (10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, size = 24)\n",
    "    plt.colorbar(aspect=4)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, size = 14)\n",
    "    plt.yticks(tick_marks, classes, size = 14)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    # Label the plot\n",
    "    for i, j in itertools.product(range(cm.shape[0]),   range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), \n",
    "                 fontsize = 20,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.grid(None)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', size = 18)\n",
    "    plt.xlabel('Predicted label', size = 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare input data\n",
    "def prepare_inputs(X_train, X_test):\n",
    "  ohe = OneHotEncoder(handle_unknown = 'ignore')\n",
    "  ohe.fit(X_train)\n",
    "  X_train_enc = ohe.transform(X_train)\n",
    "  X_test_enc = ohe.transform(X_test)\n",
    "  return X_train_enc, X_test_enc\n",
    "# prepare target\n",
    "def prepare_targets(y_train, y_test):\n",
    "  le = LabelEncoder()\n",
    "  le.fit(y_train)\n",
    "  y_train_enc = le.transform(y_train)\n",
    "  y_test_enc = le.transform(y_test)\n",
    "  return y_train_enc, y_test_enc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the data\n",
    "ros = RandomOverSampler()\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X_resampled,y_resampled,test_size=0.20, random_state = 42)\n",
    "# prepare input data\n",
    "train_X, val_X = prepare_inputs(Xtrain, Xtest)\n",
    "# prepare output data\n",
    "train_Y, val_Y = prepare_targets(Ytrain, Ytest)\n",
    "# make output 3d\n",
    "train_Y = train_Y.reshape((len(train_Y), 1, 1))\n",
    "val_Y = val_Y.reshape((len(val_Y), 1, 1))\n",
    "# define the  model\n",
    "model = Sequential()\n",
    "model.add(Dense(6, input_dim=train_X.shape[1], activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(train_X, train_Y, epochs=10, batch_size=16, verbose=2)\n",
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(val_X, val_Y, verbose=0)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(val_X)\n",
    "test_predictions = np.where(test_predictions >= 0.5, 1, 0)\n",
    "test_predictions = test_predictions.reshape(Ytest.shape[0],)\n",
    "cm = confusion_matrix(np.array(Ytest), np.array(test_predictions))\n",
    "plot_confusion_matrix(cm, classes = ['0 - Non-Purchaser', '1 - Purchaser'],\n",
    "                      title = 'Purchaser Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "val_Y = np.array(Ytest)\n",
    "test_predictions = np.array(test_predictions)\n",
    "print(\"Accuracy Score:\", metrics.accuracy_score(val_Y, test_predictions))\n",
    "print(\"Balanced Accuracy Score:\", metrics.balanced_accuracy_score(val_Y, test_predictions)) # Accuracy for imbalanced data\n",
    "print(\"ROC Score:\", metrics.roc_auc_score(val_Y, test_predictions))\n",
    "print(\"Precision\", metrics.precision_score(val_Y, test_predictions)) # Of the positives predicted, what percentage is truly positive\n",
    "print(\"Sensitivity\", metrics.recall_score(val_Y, test_predictions)) # Of all the positive cases, what percentage are predicted positive\n",
    "print(\"Specificity\", metrics.recall_score(val_Y, test_predictions, pos_label = 0)) # How well the model is at prediciting negative results\n",
    "print(\"F-Score\", metrics.f1_score(val_Y, test_predictions)) # The \"harmonic mean\" of precision and sensitivity. Good for imbalanced datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate majority and minority classes\n",
    "df_majority = data[data.is_purchasor == 0]\n",
    "df_minority = data[data.is_purchasor == 1]\n",
    " \n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True, # sample with replacement\n",
    "                                 n_samples=435253, # to match majority class\n",
    "                                 random_state=12345) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    " \n",
    "# Display new class counts\n",
    "df_upsampled.is_purchasor.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn1 = KNeighborsClassifier(n_neighbors = 1)\n",
    "knn1.fit(X_train, y_train)\n",
    "pickle.dump(knn1, open('knn1.pkl', 'wb'))\n",
    "predicted1 = knn1.predict(X_test)\n",
    "pickle.dump(predicted1, open('predicted1clicks.pkl', 'wb'))\n",
    "error_rate.append(np.mean(predicted1 != y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(y_test, predicted1)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy score:\", metrics.accuracy_score(y_test, predicted9clicks))\n",
    "print(\"Balanced accuracy score:\", metrics.balanced_accuracy_score(y_test, predicted9clicks))\n",
    "print(\"ROC AUC score:\", metrics.roc_auc_score(y_test, predicted9clicks))\n",
    "print(\"Precision score:\", metrics.precision_score(y_test, predicted9clicks))\n",
    "print(\"Sensitivity score:\", metrics.recall_score(y_test, predicted9clicks))\n",
    "print(\"Specificity score:\", metrics.recall_score(y_test, predicted9clicks, pos_label = 0))\n",
    "print(\"F-score:\", metrics.f1_score(y_test, predicted9clicks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import *\n",
    "\n",
    "# Choose optimal K using Elbow method\n",
    "cost = []\n",
    "for cluster in range(1, 10):\n",
    "    try:\n",
    "        kprototype = KPrototypes(n_jobs = -1, n_clusters = cluster, init = 'Huang', random_state = 100)\n",
    "        kprototype.fit_predict(dfMatrix, categorical = catColumnsPos)\n",
    "        cost.append(kprototype.cost_)\n",
    "        print('Cluster initiation: {}'.format(cluster))\n",
    "    except:\n",
    "        break\n",
    "# Converting the results into a dataframe and plotting them\n",
    "df_cost = pd.DataFrame({'Cluster':range(1, len(cost)+1), 'Cost':cost})\n",
    "# Data viz\n",
    "#plotnine.options.figure_size = (8, 4.8)\n",
    "(\n",
    "    ggplot(data = df_cost)+\n",
    "    geom_line(aes(x = 'Cluster',\n",
    "                  y = 'Cost'))+\n",
    "    geom_point(aes(x = 'Cluster',\n",
    "                   y = 'Cost'))+\n",
    "    geom_label(aes(x = 'Cluster',\n",
    "                   y = 'Cost',\n",
    "                   label = 'Cluster'),\n",
    "               size = 10,\n",
    "               nudge_y = 1000) +\n",
    "    labs(title = 'Optimal number of cluster with Elbow Method')+\n",
    "    xlab('Number of Clusters k')+\n",
    "    ylab('Cost')+\n",
    "    theme_minimal()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the model\n",
    "kprototype = KPrototypes(n_jobs = -1, n_clusters = 3, init = 'Huang', random_state = 0)\n",
    "\n",
    "# Fitting the model on the training data\n",
    "clusters = kprototype.fit_predict(dfMatrix, categorical = catColumnsPos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster centorid\n",
    "print(\"Cluster centroid:\", kprototype.cluster_centroids_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the cluster to the dataframe\n",
    "df['Cluster Labels'] = kprototype.labels_\n",
    "df['Segment'] = df['Cluster Labels'].map({0:'First', 1:'Second', 2:'Third'})\n",
    "# Order the cluster\n",
    "df['Segment'] = df['Segment'].astype('category')\n",
    "df['Segment'] = df['Segment'].cat.reorder_categories(['First','Second','Third'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is the data array used for clustering\n",
    "# clusters is the array of cluster labels assigned to each data point\n",
    "silhouette_avg = silhouette_score(data.drop(columns=['Segment']), clusters, metric='euclidean')\n",
    "print(\"The average silhouette score is :\", silhouette_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_n_clusters = [2,3,4,5]\n",
    "silhouette_avg = []\n",
    "for num_clusters in range_n_clusters:\n",
    " # initialise kmeans\n",
    " kmeans = KMeans(init=\"random\", n_clusters=num_clusters,n_init=10,random_state=100)\n",
    " kmeans.fit_predict(df)\n",
    " cluster_labels = kmeans.labels_\n",
    " # silhouette score\n",
    " score = silhouette_score(df, cluster_labels, metric='euclidean')\n",
    " silhouette_avg.append(score)\n",
    " print(\"Score:\", score)\n",
    " \n",
    "plt.plot(range_n_clusters,silhouette_avg,'bx-')\n",
    "plt.xlabel('Values of K') \n",
    "plt.ylabel('Silhouette score') \n",
    "plt.title('Silhouette analysis For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the k means model with our optimal K clusters \n",
    "k = 3#\n",
    "kmeans = KMeans(init=\"random\", n_clusters=k,n_init=10,random_state=100)\n",
    "y_pred = kmeans.fit_predict(df)\n",
    "df['cluster'] = y_pred\n",
    "\n",
    "# output with the cluster attached\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
